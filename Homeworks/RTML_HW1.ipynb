{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrSOeGndiRXUrClNrjbzzc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cbroh03/IntroToDL/blob/main/RTML_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import cuda\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import ssl\n",
        "device = 'cpu'"
      ],
      "metadata": {
        "id": "rnT0dgK56KT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9fDiALtM1qL"
      },
      "outputs": [],
      "source": [
        "################################    Problem 1A:\n",
        "data_path = '../../Datasets'\n",
        "normalizing_set = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transforms.ToTensor())\n",
        "images = torch.stack([img_t for img_t, _ in normalizing_set], dim=3)\n",
        "mean = images.view(3, -1).mean(dim=1)\n",
        "std = images.view(3, -1).std(dim=1)\n",
        "\n",
        "normalizer = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean,std)])\n",
        "\n",
        "cifar_train = datasets.CIFAR10(root=data_path, train=True, transform=normalizer)\n",
        "cifar_val = datasets.CIFAR10(root=data_path, train=False, transform=normalizer)\n",
        "\n",
        "train_loader = DataLoader(cifar_train, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(cifar_val, batch_size=64, shuffle=False)\n",
        "class CifarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CifarNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(32*32*3, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 32)\n",
        "        self.fc4 = nn.Linear(32, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "model = CifarNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "validation_losses = []\n",
        "validation_accuracies = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit=' batch') as pbar:\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            pbar.update()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        training_losses.append(running_loss / len(train_loader))\n",
        "        training_accuracies.append(100 * correct / total)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                predicted_labels.extend(predicted.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        validation_losses.append(running_loss / len(val_loader))\n",
        "        validation_accuracies.append(100 * correct / total)\n",
        "\n",
        "        pbar.set_postfix({'Training Loss ': training_losses[-1], 'Validation Loss ': validation_losses[-1]})\n",
        "\n",
        "print(\"Final Training Loss:\", training_losses[-1])\n",
        "print(\"Final Validation Loss:\", validation_losses[-1])\n",
        "print(\"Final Training Accuracy:\", training_accuracies[-1])\n",
        "print(\"Final Validation Accuracy:\", validation_accuracies[-1])\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, training_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, validation_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, training_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, validation_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(true_labels, predicted_labels)\n",
        "print(metrics.classification_report(true_labels, predicted_labels))\n",
        "\n",
        "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################################Problem 1B\n",
        "data_path = '../../Datasets'\n",
        "normalizing_set = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transforms.ToTensor())\n",
        "images = torch.stack([img_t for img_t, _ in normalizing_set], dim=3)\n",
        "mean = images.view(3, -1).mean(dim=1)\n",
        "std = images.view(3, -1).std(dim=1)\n",
        "\n",
        "normalizer = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean,std)])\n",
        "\n",
        "cifar_train = datasets.CIFAR10(root=data_path, train=True, transform=normalizer)\n",
        "cifar_val = datasets.CIFAR10(root=data_path, train=False, transform=normalizer)\n",
        "\n",
        "train_loader = DataLoader(cifar_train, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(cifar_val, batch_size=64, shuffle=False)\n",
        "class CifarNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CifarNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(32*32*3, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 16)\n",
        "        self.fc6 = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.fc4(x))\n",
        "        x = self.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x\n",
        "\n",
        "model = CifarNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "training_losses = []\n",
        "training_accuracies = []\n",
        "validation_losses = []\n",
        "validation_accuracies = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit=' batch') as pbar:\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            pbar.update()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        training_losses.append(running_loss / len(train_loader))\n",
        "        training_accuracies.append(100 * correct / total)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predicted_labels = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                predicted_labels.extend(predicted.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        validation_losses.append(running_loss / len(val_loader))\n",
        "        validation_accuracies.append(100 * correct / total)\n",
        "\n",
        "        pbar.set_postfix({'Training Loss ': training_losses[-1], 'Validation Loss ': validation_losses[-1]})\n",
        "\n",
        "print(\"Final Training Loss:\", training_losses[-1])\n",
        "print(\"Final Validation Loss:\", validation_losses[-1])\n",
        "print(\"Final Training Accuracy:\", training_accuracies[-1])\n",
        "print(\"Final Validation Accuracy:\", validation_accuracies[-1])\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, training_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, validation_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, training_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, validation_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(true_labels, predicted_labels)\n",
        "print(metrics.classification_report(true_labels, predicted_labels))\n",
        "\n",
        "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D215ZaRlWlh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import cuda\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler as SS\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "59E1KeNX6P19"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################            Problem 2A\n",
        "full_df = pd.read_csv('../../Datasets/house-train.csv')\n",
        "\n",
        "usefull_cols = ['OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath',\n",
        "                'YearBuilt', 'YearRemodAdd', 'Fireplaces', 'LotFrontage','WoodDeckSF',\n",
        "                'OpenPorchSF', 'ExterQual', 'Neighborhood', 'MSZoning', 'Alley', 'LotShape',\n",
        "                'LandContour','Condition1','HouseStyle','MasVnrType','SaleCondition', 'SalePrice']\n",
        "\n",
        "unscaled_df = full_df[usefull_cols].copy()\n",
        "\n",
        "unscaled_X = unscaled_df.drop(['SalePrice'], axis=1)\n",
        "unscaled_X = unscaled_X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "unscaled_Y = unscaled_df['SalePrice'].values.reshape(-1,1)\n",
        "\n",
        "X = SS().fit_transform(unscaled_X)\n",
        "Y = SS().fit_transform(unscaled_Y)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.8, random_state=7)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "train = TensorDataset(x_train, y_train)\n",
        "val = TensorDataset(x_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val, batch_size=64, shuffle=False)\n",
        "class RegressNet(nn.Module):\n",
        "    def __init__(self, input):\n",
        "        super(RegressNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = RegressNet(input=x_train.shape[1]).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0008)\n",
        "\n",
        "epochs = 100\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit=' batch') as pbar:\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            pbar.update()\n",
        "\n",
        "        training_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        validation_total = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                validation_total += ((outputs - labels) ** 2).sum().item()\n",
        "\n",
        "        validation_losses.append(running_loss / len(val_loader))\n",
        "        rmse = np.sqrt(validation_total / len(val_loader.dataset))\n",
        "\n",
        "        pbar.set_postfix({'Training Loss ': training_losses[-1], 'Validation Loss ': validation_losses[-1], 'Validation RMSE ': rmse})\n",
        "\n",
        "\n",
        "print(\"Final Training Loss:\", training_losses[-1])\n",
        "print(\"Final Validation Loss:\", validation_losses[-1])\n",
        "print(\"Final Validation RMSE:\", rmse)\n",
        "plt.plot(training_losses, label='Training Loss')\n",
        "plt.plot(validation_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k6H_jH72etk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### Problem 2B\n",
        "unscaled_df = full_df[usefull_cols].copy()\n",
        "\n",
        "unencoded_unscaled_X = pd.get_dummies(unscaled_df, columns=['Neighborhood','MSZoning','Alley','LotShape','LandContour','Condition1','HouseStyle','MasVnrType','SaleCondition'])\n",
        "encoded_unscaled_X = unencoded_unscaled_X.replace({True: 1, False: 0})\n",
        "encoded_unscaled_X = encoded_unscaled_X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "unscaled_Y = unscaled_df['SalePrice'].values.reshape(-1,1)\n",
        "\n",
        "X = SS().fit_transform(encoded_unscaled_X)\n",
        "Y = SS().fit_transform(unscaled_Y)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.8, random_state=7)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "train = TensorDataset(x_train, y_train)\n",
        "val = TensorDataset(x_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val, batch_size=64, shuffle=False)\n",
        "class RegressNet(nn.Module):\n",
        "    def __init__(self, input):\n",
        "        super(RegressNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = RegressNet(input=x_train.shape[1]).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0008)\n",
        "\n",
        "epochs = 100\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit=' batch') as pbar:\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            pbar.update()\n",
        "\n",
        "        training_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        validation_total = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                validation_total += ((outputs - labels) ** 2).sum().item()\n",
        "\n",
        "        validation_losses.append(running_loss / len(val_loader))\n",
        "        rmse = np.sqrt(validation_total / len(val_loader))\n",
        "\n",
        "        pbar.set_postfix({'Training Loss ': training_losses[-1], 'Validation Loss ': validation_losses[-1], 'Validation RMSE ': rmse})\n",
        "\n",
        "print(\"Final Training Loss:\", training_losses[-1])\n",
        "print(\"Final Validation Loss:\", validation_losses[-1])\n",
        "print(\"Final Validation RMSE:\", rmse)\n",
        "plt.plot(training_losses, label='Training Loss')\n",
        "plt.plot(validation_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2GuVSRwC5Xik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####   problem 2C\n",
        "\n",
        "unscaled_df = full_df[usefull_cols].copy()\n",
        "\n",
        "unencoded_unscaled_X = pd.get_dummies(unscaled_df, columns=['Neighborhood','MSZoning','Alley','LotShape','LandContour','Condition1','HouseStyle','MasVnrType','SaleCondition'])\n",
        "encoded_unscaled_X = unencoded_unscaled_X.replace({True: 1, False: 0})\n",
        "encoded_unscaled_X = encoded_unscaled_X.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "\n",
        "unscaled_Y = unscaled_df['SalePrice'].values.reshape(-1,1)\n",
        "\n",
        "X = SS().fit_transform(encoded_unscaled_X)\n",
        "Y = SS().fit_transform(unscaled_Y)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(X, Y, train_size=0.8, random_state=7)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "train = TensorDataset(x_train, y_train)\n",
        "val = TensorDataset(x_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val, batch_size=64, shuffle=False)\n",
        "\n",
        "class RegressNet(nn.Module):\n",
        "    def __init__(self, input):\n",
        "        super(RegressNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 16)\n",
        "        self.fc5 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "model = RegressNet(input=x_train.shape[1]).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0008)\n",
        "\n",
        "epochs = 100\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit=' batch') as pbar:\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            pbar.update()\n",
        "\n",
        "        training_losses.append(running_loss / len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        validation_total = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                validation_total += ((outputs - labels) ** 2).sum().item()\n",
        "\n",
        "        validation_losses.append(running_loss / len(val_loader))\n",
        "        rmse = np.sqrt(validation_total / len(val_loader.dataset))\n",
        "\n",
        "        pbar.set_postfix({'Training Loss ': training_losses[-1], 'Validation Loss ': validation_losses[-1], 'Validation RMSE ': rmse})\n",
        "\n",
        "print(\"Final Training Loss:\", training_losses[-1])\n",
        "print(\"Final Validation Loss:\", validation_losses[-1])\n",
        "print(\"Final Validation RMSE:\", rmse)\n",
        "plt.plot(training_losses, label='Training Loss')\n",
        "plt.plot(validation_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_FWNkN7S5iD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}